version: '3.7'

services:
  s3server:
    # profiles: ["all", "demo"]
    image: localstack/localstack-full:0.12.15
    ports:
      - "4563-4584:4563-4584"
    env_file:
      - .s3.env
    volumes:
      - ./aws.setup.d:/docker-entrypoint-initaws.d
      - ./data:/initdata
      - ./.s3-mount:/tmp/localstack
    networks:
      datamesh:

  azserver:
    image: mcr.microsoft.com/azure-storage/azurite:3.15.0
    command:
      - "azurite"
      - "-l"
      - "/workspace"
      - "-d"
      - "/workspace/logs/debug.log"
      - "--blobHost"
      - "0.0.0.0"
      - "--queueHost"
      - "0.0.0.0"
      - "--tableHost"
      - "0.0.0.0"
      - "--loose"
    ports:
      - 10000:10000
      - 10001:10001
      - 10002:10002
    env_file:
      - .azure.env
    volumes:
      - ./.az-mount/workspace:/workspace
    networks:
      datamesh:
        aliases:
          - devstoreaccount1.blob.azserver

  delta:
    # profiles: ["all", "demo"]
    image: deltaio/delta-sharing-server:0.3.0
    command: ["--config", "/opt/docker/conf/delta-sharing.yml"]
    ports:
      - 38080:8080
    env_file:
      - .delta.env
    volumes:
      - ./config/delta/:/opt/docker/conf/
    networks:
      datamesh:

  sparkprepare:
    # profiles: ["all"]
    build:
      context: ./docker/aws-spark
      dockerfile: Dockerfile
      args:
        - SPARK_VERSION=3.1.1
    env_file:
      - .s3.env
    environment:
      - SPARK_NO_DAEMONIZE=true
    entrypoint:
      - /usr/bin/tail
      - "-f"
      - /dev/null
    networks:
      datamesh:

  sparkshell:
    # profiles: ["all", "demo"]
    build:
      context: ./docker/aws-spark
      dockerfile: Dockerfile
      args:
        - SPARK_VERSION=3.1.1
    environment:
      - SPARK_NO_DAEMONIZE=true
    entrypoint:
      - /usr/bin/tail
      - "-f"
      - /dev/null
    volumes:
      - ./config/sharing/:/opt/delta/conf/
    networks:
      datamesh:

  python:
    # profiles: ["all", "demo"]
    build:
      context: ./docker/python-delta
      dockerfile: Dockerfile
    tty: true
    volumes:
      - ./config/sharing/:/opt/delta/conf/
    networks:
      datamesh:

  jupyter:
    # profiles: ["all", "demo"]
    image: almondsh/almond:latest
    environment:
      - SPARK_CONF_DEFAULTS=/opt/spark/conf/spark.conf
      - JUPYTER_ENABLE_LAB=true
    command:
      - "start-notebook.sh"
      - "--NotebookApp.token=datamesh"
      - "--NotebookApp.notebook_dir=/home/jovyan/work/notebooks"
    ports:
      - 18888:8888
    volumes:
      - ./notebooks/:/home/jovyan/work/notebooks/
      - ./config/sharing/:/opt/delta/conf/
      - ./config/spark/:/opt/spark/conf/
    networks:
      datamesh:

networks:
  datamesh: